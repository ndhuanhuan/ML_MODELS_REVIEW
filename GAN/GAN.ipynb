{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs are able to learn how to model the input distribution by training two\n",
    "competing (and cooperating) networks referred to as generator and discriminator\n",
    "(sometimes known as critic). The role of the generator is to keep on figuring out\n",
    "how to generate fake data or signals (this includes, audio and images) that can\n",
    "fool the discriminator. Meanwhile, the discriminator is trained to distinguish\n",
    "between fake and real signals. As the training progresses, the discriminator will\n",
    "no longer be able to see the difference between the synthetically generated data\n",
    "and the real ones. From there, the discriminator can be discarded, and the generator\n",
    "can now be used to create new realistic signals that have never been observed before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one thing we'll find is that the most challenging aspect is how do we achieve stable training\n",
    "of the generator-discriminator network? There must be a healthy competition\n",
    "between the generator and discriminator in order for both networks to be able\n",
    "to learn simultaneously. Since the loss function is computed from the output\n",
    "of the discriminator, its parameters update is fast. When the discriminator\n",
    "converges faster, the generator no longer receives sufficient gradient updates for\n",
    "its parameters and fails to converge. Other than being hard to train, GANs can also\n",
    "suffer from either a partial or total modal collapse, a situation wherein the generator\n",
    "is producing almost similar outputs for different latent encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principles of GANs\n",
    "a GAN is made up of two networks, a generator, and\n",
    "a discriminator. The input to the generator is noise, and the output is a synthesized\n",
    "signal. \n",
    "\n",
    "Meanwhile, the discriminator's input will be either a real or a synthesized\n",
    "signal. Genuine signals come from the true sampled data, while the fake signals\n",
    "come from the generator. All of the valid signals are labeled 1.0 (that is, 100%\n",
    "probability of being real) while all the synthesized signals are labeled 0.0 (that\n",
    "is, 0% probability of being real). Since the labeling process is automated, GANs\n",
    "are still considered part of the unsupervised learning approach in deep learning.\n",
    "\n",
    "The objective of the discriminator is to learn from this supplied dataset on how\n",
    "to distinguish real signals from fake signals. During this part of GAN training,\n",
    "only the discriminator parameters will be updated. Like a typical binary classifier,\n",
    "the discriminator is trained to predict on a range of 0.0 to 1.0 in confidence values\n",
    "on how close a given input signal is to the true one. However, this is only half\n",
    "of the story.\n",
    "\n",
    "At regular intervals, the generator will pretend that its output is a genuine signal\n",
    "and will ask the GAN to label it as 1.0. When the fake signal is then presented\n",
    "to the discriminator, naturally it will be classified as fake with a label close to 0.0.\n",
    "The optimizer computes the generator parameter updates based on the presented\n",
    "label (that is, 1.0). It also takes its own prediction into account when training\n",
    "on this new data. In other words, the discriminator has some doubt about its\n",
    "prediction, and so, GANs takes that into consideration. This time, GANs will let\n",
    "the gradients backpropagate from the last layer of the discriminator down to the\n",
    "first layer of the generator. However, in most practices, during this phase of training,\n",
    "the discriminator parameters are temporarily frozen. The generator will use the\n",
    "gradients to update its parameters and improve its ability to synthesize fake signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN\n",
    "• Use of strides > 1 convolution instead of MaxPooling2D or UpSampling2D.\n",
    "With strides > 1, the CNN learns how to resize the feature maps.\n",
    "• Avoid using Dense layers. Use CNN in all layers. The Dense layer is utilized\n",
    "only as the first layer of the generator to accept the z-vector. The output of the\n",
    "Dense layer is resized and becomes the input of the succeeding CNN layers.\n",
    "• Use of Batch Normalization (BN) to stabilize learning by normalizing\n",
    "the input to each layer to have zero mean and unit variance. No BN\n",
    "in the generator output layer and discriminator input layer. In the\n",
    "implementation example to be presented here, no batch normalization\n",
    "is used in the discriminator.\n",
    "• Rectified Linear Unit (ReLU) is used in all layers of the generator except in\n",
    "the output layer where the tanh activation is utilized. In the implementation\n",
    "example to be presented here, sigmoid is used instead of tanh in the output\n",
    "of the generator since it generally results in a more stable training for\n",
    "MNIST digits.\n",
    "• Use of Leaky ReLU in all layers of the discriminator. Unlike ReLU, instead of\n",
    "zeroing out all outputs when the input is less than zero, Leaky ReLU generates\n",
    "a small gradient equal to alpha × input. In the following example, alpha = 0.2.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
