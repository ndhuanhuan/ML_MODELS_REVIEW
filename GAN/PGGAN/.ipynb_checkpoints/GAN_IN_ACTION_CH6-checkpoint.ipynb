{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upscale_layer(layer, upscale_factor):\n",
    "\t'''\n",
    "\tUpscales layer (tensor) by the factor (int) where\n",
    "      the tensor is [group, height, width, channels]\n",
    "\t'''\n",
    "\theight, width = layer.get_shape()[1:3]\n",
    "\tsize = (upscale_factor * height, upscale_factor * width)\n",
    "\tupscaled_layer = tf.image.resize_nearest_neighbor(layer, size)\n",
    "\treturn upscaled_layer\n",
    "\n",
    "def smoothly_merge_last_layer(list_of_layers, alpha):\n",
    "\t'''\n",
    "\tSmoothly merges in a layer based on a threshold value alpha.\n",
    "\tThis function assumes: that all layers are already in RGB. \n",
    "\tThis is the function for the Generator.\n",
    "\t:list_of_layers\t:\titems should be tensors ordered by size\n",
    "\t:alpha \t\t\t: \tfloat \\in (0,1)\n",
    "\t'''\n",
    "\t# Hint!\n",
    "  # If you are using pure Tensorflow rather than Keras, always remember scope\n",
    "\tlast_fully_trained_layer = list_of_layers[-2]\n",
    "\t# now we have the originally trained layer\n",
    "\tlast_layer_upscaled = upscale_layer(last_fully_trained_layer, 2)\n",
    "\n",
    "\t# this is the newly added layer not yet fully trained\n",
    "\tlarger_native_layer = list_of_layers[-1]\n",
    "\n",
    "\t# This makes sure we can run the merging code\n",
    "\tassert larger_native_layer.get_shape() == last_layer_upscaled.get_shape()\n",
    "\n",
    "\t# This code block should take advantage of broadcasting\n",
    "\tnew_layer = (1-alpha) * upscaled_layer + larger_native_layer * alpha\n",
    "\n",
    "\treturn new_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_std_layer(layer, group_size=4):\n",
    "  '''\n",
    "  Will calculate minibatch standard deviation for a layer.\n",
    "  Will do so under a pre-specified tf-scope with Keras.\n",
    "  Assumes layer is a float32 data type. Else needs validation/casting.\n",
    "  NOTE: there is a more efficient way to do this in Keras, but just for\n",
    "  clarity and alignment with major implementations (for understanding) \n",
    "  this was done more explicitly. Try this as an exercise.\n",
    "  '''\n",
    "  # Hint!\n",
    "  # If you are using pure Tensorflow rather than Keras, always remember scope\n",
    "  # minibatch group must be divisible by (or <=) group_size\n",
    "  group_size = K.backend.minimum(group_size, tf.shape(layer)[0])\n",
    "\n",
    "  # just getting some shape information so that we can use\n",
    "  # them as shorthand as well as to ensure defaults\n",
    "  shape = list(K.int_shape(input))\n",
    "  shape[0] = tf.shape(input)[0]\n",
    "\n",
    "  # Reshaping so that we operate on the level of the minibatch\n",
    "  # in this code we assume the layer to be:\n",
    "  # [Group (G), Minibatch (M), Width (W), Height (H) , Channel (C)]\n",
    "  # but be careful different implementations use the Theano specific\n",
    "  # order instead\n",
    "  minibatch = K.backend.reshape(layer, (group_size, -1, shape[1], shape[2], shape[3]))\n",
    "\n",
    "  # Center the mean over the group [M, W, H, C]\n",
    "  minibatch -= tf.reduce_mean(minibatch, axis=0, keepdims=True)\n",
    "  # Calculate the variance of the group [M, W, H, C]\n",
    "  minibatch = tf.reduce_mean(K.backend.square(minibatch), axis = 0)\n",
    "  # Calculate the standard deviation over the group [M,W,H,C]\n",
    "  minibatch = K.backend.square(minibatch + 1e8)\n",
    "  # Take average over feature maps and pixels [M,1,1,1]\n",
    "  minibatch = tf.reduce_mean(minibatch, axis=[1,2,4], keepdims=True)\n",
    "  # Add as a layer for each group and pixels\n",
    "  minibatch = K.backend.tile(minibatch, [group_size, 1, shape[2], shape[3]])\n",
    "  # Append as a new feature map\n",
    "  return K.backend.concatenate([layer, minibatch], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize_learning_rate(shape, gain, fan_in=None):\n",
    "    '''\n",
    "    This adjusts the weights of every layer by the constant from\n",
    "    He's initializer so that we adjust for the variance in the dynamic range\n",
    "    in different features\n",
    "    shape   :   shape of tensor (layer): these are the dimensions of each layer.\n",
    "    For example, [4,4,48,3]. In this case, \n",
    "        [kernel_size, kernel_size, number_of_filters, feature_maps]. \n",
    "        But this will depend slightly on your implementation.\n",
    "    gain    :   typically sqrt(2)\n",
    "    fan_in  :   adjustment for the number of incoming connections as per Xavier's / He's initialization \n",
    "    '''\n",
    "    # Default value is product of all the shape dimension minus the feature maps dim -- this gives us the number of incoming connections per neuron\n",
    "    if fan_in is None: fan_in = np.prod(shape[:-1])\n",
    "    # This uses He's initialization constant (He et al, 2015)\n",
    "    std = gain / K.sqrt(fan_in)\n",
    "    # creates a constant out of the adjustment\n",
    "    wscale = K.constant(std, name='wscale', dtype=np.float32)\n",
    "    # gets values for weights and then uses broadcasting to apply the adjustment\n",
    "    adjusted_weights = K.get_value('layer', shape=shape, \n",
    "            initializer=tf.initializers.random_normal()) * wscale\n",
    "    return adjusted_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixelwise_feat_norm(inputs, **kwargs):\n",
    "\t'''\n",
    "\tUses pixelwise feature normalization as proposed by\n",
    "\tKrizhevsky et at. 2012. Returns the input normalized\n",
    "\t:inputs \t: \tKeras / TF Layers \n",
    "\t'''\n",
    "\tnormalization_constant = K.backend.sqrt(K.backend.mean(\n",
    "\t\t\t\t\tinputs**2, axis=-1, keepdims=True) + 1.0e-8)\n",
    "\treturn inputs / normalization_constant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
