# Progressive GAN Basics
## Progressive growing and smoothing of higher-resolution layers
In technical terms, we are going from a few low-resolution convolutional layers to many high-resolution ones as we train. Thus, we first train the early layers and only then introduce a higher-resolution layer, where it is harder to navigate the loss space. We go from something simple—for example, 4 × 4 trained for several steps—to something more complex—for example, 1024 × 1024 trained for several epochs.

The problem in this scenario is that upon introducing even one more layer at a time (for example, from 4 × 4 to 8 × 8), we are still introducing a massive shock to the training. What the PGGAN authors do instead is smoothly fade in those layers, in order to give the system time to adapt to the higher resolution.

However, rather than immediately jumping to this resolution, we smoothly fade in this new layer with higher resolution by a parameter alpha (α), which is between 0 and 1. Alpha affects how much we use either the old—but upscaled—layer or the natively larger one. On the side of the D, we simply shrink by 0.5x to allow for smoothly injecting the trained layer for discrimination.

## Mini-batch standard deviation
Let’s recall the issue of mode collapse, which occurs when the GAN learns how to create a few good examples or only slight permutations on them. We generally want to produce the faces of all the people in the real dataset, maybe not just one picture of one woman.

Therefore, Karras et al. created a way for the Discriminator to tell whether the samples it is getting are varied enough. In essence, we calculate a single extra scalar statistic for the Discriminator. This statistic is the standard deviation of all the pixels in the mini-batch that are generated by the Generator or that come from the real data. That is an amazingly simple and elegant solution: now all the Discriminator needs to learn is that if the standard deviation is low in the images from the batch it is evaluating, the image is likely fake, because the real data has more variance. The Generator has no choice but to increase the variance of the generated samples to have a chance to fool the Discriminator

Moving beyond the intuition, the technical implementation is straightforward as it applies only to the Discriminator. Given that we also want to minimize the number of trainable parameters, we include only a single extra number, which seems to be enough. This number is appended as a feature map—think dimension or the last number in the tf.shape list.

1. [4D -> 3D] We compute the standard deviation across all the images in the batch, across all the remaining channels—height, width, and color. We then get a single image with standard deviations for each pixel and each channel.
2. [3D -> 2D] We average the standard deviations across all channels—to get a single feature map or matrix of standard deviations for that pixel, but with a collapsed color channel.
3. [2D -> Scalar/0D] We average the standard deviations for all pixels within the preceding matrix to get a single scalar value.

## Equalized learning rate
Probably a hack:
we need to ensure that all the weights (w) are normalized (w’) to be within a certain range such that w’ = w/c by a constant c that is different for each layer, depending on the shape of the weight matrix. This also ensures that if any parameters need to take bigger steps to reach optimum—because they tend to vary more—these relevant parameters can do that.

## Pixel-wise feature normalization in the generator
Note that most networks are using some form of normalization. Typically, they use either batch normalization or a virtual version of this technique.

Unfortunately, batch normalization is too memory intensive at our resolution. We have to come up with something that allows us to work with a few examples—that fit into our GPU memory with the two network graphs—but still works well. Now we understand where the need for pixel-wise feature normalization comes from and why we use it.

If we jump into the algorithm, pixel normalization takes activation magnitude at each layer just before the input is fed into the next layer.

For each feature map do

1. Take the pixel value of that feature map (fm) at a position (x, y).
2. Construct a vector for each (x, y), where
 - v0,0 = [(0,0) value for fm1, (0,0) value for fm2, ...., (0,0) value for fmn]
 - v0,1 = [(0,1) value for fm1, (0,1) value for fm2, ...., (0,1) value for fmn] ...
 - vn,n = [(n,n) value for fm1, (n,n) value for fm2, ...., (n,n) value for fmn]
3. Normalize each vector vi,i as defined in step 2 to have a unit norm; call it ni,i.
4. Pass that in the original tensor shape to the next layer.

End for
