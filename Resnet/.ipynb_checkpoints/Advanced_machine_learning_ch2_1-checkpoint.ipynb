{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet introduced the concept of residual learning which enabled it to build\n",
    "very deep networks by addressing the vanishing gradient problem in deep\n",
    "convolutional networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DenseNet improved the ResNet technique further by allowing every convolution\n",
    "to have direct access to inputs, and lower layer feature maps. It's also managed\n",
    "to keep the number of parameters low in deep networks by utilizing both the\n",
    "Bottleneck and Transition layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "important feature of Keras called the Functional API. This API acts\n",
    "as an alternative method for building networks in Keras and enables us to build\n",
    "more complex networks that cannot be accomplished by the sequential mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Functional API is guided by the following two concepts:\n",
    "• A layer is an instance that accepts a tensor as an argument. The output of\n",
    "a layer is another tensor. To build a model, the layer instances are objects that\n",
    "are chained to one another through both input and output tensors. This will\n",
    "have similar end-result as would stacking multiple layers in the sequential\n",
    "model have. However, using layer instances makes it easier for models to\n",
    "have either auxiliary or multiple inputs and outputs since the input/output\n",
    "of each layer will be readily accessible.\n",
    "• A model is a function between one or more input tensors and output tensors.\n",
    "In between the model input and output, tensors are the layer instances that\n",
    "are chained to one another by layer input and output tensors. A model is,\n",
    "therefore, a function of one or more input layers and one or more output\n",
    "layers. The model instance formalizes the computational graph on how\n",
    "the data flows from input(s) to output(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll find Listing 2.1.1, cnn-functional-2.1.1.py, as follows. This shows us\n",
    "how we can convert the cnn-mnist-1.4.1.py code using the functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5770      \n",
      "=================================================================\n",
      "Total params: 80,266\n",
      "Trainable params: 80,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 24s 393us/step - loss: 0.2836 - acc: 0.9104 - val_loss: 0.0587 - val_acc: 0.9818\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0775 - acc: 0.9767 - val_loss: 0.0380 - val_acc: 0.9876\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 0.0538 - acc: 0.9829 - val_loss: 0.0299 - val_acc: 0.9897\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 14s 225us/step - loss: 0.0426 - acc: 0.9864 - val_loss: 0.0281 - val_acc: 0.9907\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 14s 225us/step - loss: 0.0358 - acc: 0.9886 - val_loss: 0.0278 - val_acc: 0.9905\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0321 - acc: 0.9896 - val_loss: 0.0260 - val_acc: 0.9909\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 14s 229us/step - loss: 0.0276 - acc: 0.9909 - val_loss: 0.0239 - val_acc: 0.9916\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0250 - acc: 0.9920 - val_loss: 0.0219 - val_acc: 0.9928\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0213 - acc: 0.9927 - val_loss: 0.0226 - val_acc: 0.9921\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 0.0186 - acc: 0.9941 - val_loss: 0.0237 - val_acc: 0.9922\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.0175 - acc: 0.9944 - val_loss: 0.0200 - val_acc: 0.9941\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 16s 261us/step - loss: 0.0168 - acc: 0.9945 - val_loss: 0.0215 - val_acc: 0.9935\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0136 - acc: 0.9955 - val_loss: 0.0251 - val_acc: 0.9917\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0137 - acc: 0.9954 - val_loss: 0.0220 - val_acc: 0.9938\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 15s 244us/step - loss: 0.0115 - acc: 0.9960 - val_loss: 0.0252 - val_acc: 0.9925\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 14s 232us/step - loss: 0.0111 - acc: 0.9963 - val_loss: 0.0226 - val_acc: 0.9936\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 14s 225us/step - loss: 0.0108 - acc: 0.9965 - val_loss: 0.0226 - val_acc: 0.9938\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.0095 - acc: 0.9967 - val_loss: 0.0237 - val_acc: 0.9928\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.0090 - acc: 0.9970 - val_loss: 0.0267 - val_acc: 0.9929\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 15s 243us/step - loss: 0.0088 - acc: 0.9970 - val_loss: 0.0234 - val_acc: 0.9936\n",
      "10000/10000 [==============================] - 1s 72us/step\n",
      "\n",
      "Test accuracy: 99.4%\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# from sparse label to categorical\n",
    "num_labels = len(np.unique(y_train))\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# reshape and normalize input images\n",
    "image_size = x_train.shape[1]\n",
    "x_train = np.reshape(x_train,[-1, image_size, image_size, 1])\n",
    "x_test = np.reshape(x_test,[-1, image_size, image_size, 1])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# network parameters\n",
    "input_shape = (image_size, image_size, 1)\n",
    "batch_size = 128\n",
    "kernel_size = 3\n",
    "filters = 64\n",
    "dropout = 0.3\n",
    "\n",
    "# use functional API to build cnn layers\n",
    "inputs = Input(shape=input_shape)\n",
    "y = Conv2D(filters=filters,\n",
    "           kernel_size=kernel_size,\n",
    "           activation='relu')(inputs)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Conv2D(filters=filters,\n",
    "           kernel_size=kernel_size,\n",
    "           activation='relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Conv2D(filters=filters,\n",
    "           kernel_size=kernel_size,\n",
    "           activation='relu')(y)\n",
    "# image to vector before connecting to dense layer\n",
    "y = Flatten()(y)\n",
    "# dropout regularization\n",
    "y = Dropout(dropout)(y)\n",
    "outputs = Dense(num_labels, activation='softmax')(y)\n",
    "\n",
    "# build the model by supplying inputs/outputs\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "# network model in text\n",
    "model.summary()\n",
    "\n",
    "# classifier loss, Adam optimizer, classifier accuracy\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train the model with input images and labels\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          epochs=20,\n",
    "          batch_size=batch_size)\n",
    "\n",
    "# model accuracy on test dataset\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
