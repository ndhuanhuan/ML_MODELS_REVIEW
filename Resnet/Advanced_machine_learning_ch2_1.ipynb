{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet introduced the concept of residual learning which enabled it to build\n",
    "very deep networks by addressing the vanishing gradient problem in deep\n",
    "convolutional networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DenseNet improved the ResNet technique further by allowing every convolution\n",
    "to have direct access to inputs, and lower layer feature maps. It's also managed\n",
    "to keep the number of parameters low in deep networks by utilizing both the\n",
    "Bottleneck and Transition layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "important feature of Keras called the Functional API. This API acts\n",
    "as an alternative method for building networks in Keras and enables us to build\n",
    "more complex networks that cannot be accomplished by the sequential mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Functional API is guided by the following two concepts:\n",
    "• A layer is an instance that accepts a tensor as an argument. The output of\n",
    "a layer is another tensor. To build a model, the layer instances are objects that\n",
    "are chained to one another through both input and output tensors. This will\n",
    "have similar end-result as would stacking multiple layers in the sequential\n",
    "model have. However, using layer instances makes it easier for models to\n",
    "have either auxiliary or multiple inputs and outputs since the input/output\n",
    "of each layer will be readily accessible.\n",
    "• A model is a function between one or more input tensors and output tensors.\n",
    "In between the model input and output, tensors are the layer instances that\n",
    "are chained to one another by layer input and output tensors. A model is,\n",
    "therefore, a function of one or more input layers and one or more output\n",
    "layers. The model instance formalizes the computational graph on how\n",
    "the data flows from input(s) to output(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll find Listing 2.1.1, cnn-functional-2.1.1.py, as follows. This shows us\n",
    "how we can convert the cnn-mnist-1.4.1.py code using the functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5770      \n",
      "=================================================================\n",
      "Total params: 80,266\n",
      "Trainable params: 80,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 24s 393us/step - loss: 0.2836 - acc: 0.9104 - val_loss: 0.0587 - val_acc: 0.9818\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0775 - acc: 0.9767 - val_loss: 0.0380 - val_acc: 0.9876\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 0.0538 - acc: 0.9829 - val_loss: 0.0299 - val_acc: 0.9897\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 14s 225us/step - loss: 0.0426 - acc: 0.9864 - val_loss: 0.0281 - val_acc: 0.9907\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 14s 225us/step - loss: 0.0358 - acc: 0.9886 - val_loss: 0.0278 - val_acc: 0.9905\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0321 - acc: 0.9896 - val_loss: 0.0260 - val_acc: 0.9909\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 14s 229us/step - loss: 0.0276 - acc: 0.9909 - val_loss: 0.0239 - val_acc: 0.9916\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0250 - acc: 0.9920 - val_loss: 0.0219 - val_acc: 0.9928\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0213 - acc: 0.9927 - val_loss: 0.0226 - val_acc: 0.9921\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 0.0186 - acc: 0.9941 - val_loss: 0.0237 - val_acc: 0.9922\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.0175 - acc: 0.9944 - val_loss: 0.0200 - val_acc: 0.9941\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 16s 261us/step - loss: 0.0168 - acc: 0.9945 - val_loss: 0.0215 - val_acc: 0.9935\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0136 - acc: 0.9955 - val_loss: 0.0251 - val_acc: 0.9917\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0137 - acc: 0.9954 - val_loss: 0.0220 - val_acc: 0.9938\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 15s 244us/step - loss: 0.0115 - acc: 0.9960 - val_loss: 0.0252 - val_acc: 0.9925\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 14s 232us/step - loss: 0.0111 - acc: 0.9963 - val_loss: 0.0226 - val_acc: 0.9936\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 14s 225us/step - loss: 0.0108 - acc: 0.9965 - val_loss: 0.0226 - val_acc: 0.9938\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.0095 - acc: 0.9967 - val_loss: 0.0237 - val_acc: 0.9928\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.0090 - acc: 0.9970 - val_loss: 0.0267 - val_acc: 0.9929\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 15s 243us/step - loss: 0.0088 - acc: 0.9970 - val_loss: 0.0234 - val_acc: 0.9936\n",
      "10000/10000 [==============================] - 1s 72us/step\n",
      "\n",
      "Test accuracy: 99.4%\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# from sparse label to categorical\n",
    "num_labels = len(np.unique(y_train))\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# reshape and normalize input images\n",
    "image_size = x_train.shape[1]\n",
    "x_train = np.reshape(x_train,[-1, image_size, image_size, 1])\n",
    "x_test = np.reshape(x_test,[-1, image_size, image_size, 1])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# network parameters\n",
    "input_shape = (image_size, image_size, 1)\n",
    "batch_size = 128\n",
    "kernel_size = 3\n",
    "filters = 64\n",
    "dropout = 0.3\n",
    "\n",
    "# use functional API to build cnn layers\n",
    "inputs = Input(shape=input_shape)\n",
    "y = Conv2D(filters=filters,\n",
    "           kernel_size=kernel_size,\n",
    "           activation='relu')(inputs)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Conv2D(filters=filters,\n",
    "           kernel_size=kernel_size,\n",
    "           activation='relu')(y)\n",
    "y = MaxPooling2D()(y)\n",
    "y = Conv2D(filters=filters,\n",
    "           kernel_size=kernel_size,\n",
    "           activation='relu')(y)\n",
    "# image to vector before connecting to dense layer\n",
    "y = Flatten()(y)\n",
    "# dropout regularization\n",
    "y = Dropout(dropout)(y)\n",
    "outputs = Dense(num_labels, activation='softmax')(y)\n",
    "\n",
    "# build the model by supplying inputs/outputs\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "# network model in text\n",
    "model.summary()\n",
    "\n",
    "# classifier loss, Adam optimizer, classifier accuracy\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train the model with input images and labels\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          epochs=20,\n",
    "          batch_size=batch_size)\n",
    "\n",
    "# model accuracy on test dataset\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Y-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P43\n",
    "The Y-Network uses the same input\n",
    "twice, both on the left and right CNN branches. The network combines the results\n",
    "using concatenate layer. The merge operation concatenate is similar to stacking\n",
    "two tensors of the same shape along the concatenation axis to form one tensor.\n",
    "\n",
    "filters *= 2\n",
    "Classic cnn pattern, deeper layers got more filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 28, 28, 32)   320         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 28, 28, 32)   320         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 28, 28, 32)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 28, 28, 32)   0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 14, 14, 32)   0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 14, 14, 32)   0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 14, 14, 64)   18496       max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 14, 14, 64)   18496       max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 14, 14, 64)   0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 14, 14, 64)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 7, 7, 64)     0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 7, 7, 64)     0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 7, 7, 128)    73856       max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 7, 7, 128)    73856       max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 7, 7, 128)    0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 7, 7, 128)    0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 3, 3, 128)    0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 3, 3, 128)    0           dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 3, 3, 256)    0           max_pooling2d_9[0][0]            \n",
      "                                                                 max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2304)         0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 2304)         0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           23050       dropout_14[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 208,394\n",
      "Trainable params: 208,394\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 69s 1ms/step - loss: 0.1745 - acc: 0.9445 - val_loss: 0.1274 - val_acc: 0.9875\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 59s 979us/step - loss: 0.0669 - acc: 0.9787 - val_loss: 0.0837 - val_acc: 0.9917\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 59s 978us/step - loss: 0.0547 - acc: 0.9830 - val_loss: 0.0814 - val_acc: 0.9903\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 58s 974us/step - loss: 0.0503 - acc: 0.9841 - val_loss: 0.0834 - val_acc: 0.9907\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 58s 967us/step - loss: 0.0435 - acc: 0.9865 - val_loss: 0.0549 - val_acc: 0.9932\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 58s 959us/step - loss: 0.0401 - acc: 0.9871 - val_loss: 0.0732 - val_acc: 0.9949\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 58s 958us/step - loss: 0.0384 - acc: 0.9878 - val_loss: 0.0440 - val_acc: 0.9931\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 58s 959us/step - loss: 0.0367 - acc: 0.9886 - val_loss: 0.0568 - val_acc: 0.9938\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 58s 959us/step - loss: 0.0337 - acc: 0.9892 - val_loss: 0.0439 - val_acc: 0.9924\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 58s 967us/step - loss: 0.0358 - acc: 0.9890 - val_loss: 0.0417 - val_acc: 0.9942\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 78s 1ms/step - loss: 0.0314 - acc: 0.9903 - val_loss: 0.0390 - val_acc: 0.9931\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 85s 1ms/step - loss: 0.0326 - acc: 0.9899 - val_loss: 0.0473 - val_acc: 0.9943\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 58s 970us/step - loss: 0.0311 - acc: 0.9902 - val_loss: 0.0409 - val_acc: 0.9933\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 102s 2ms/step - loss: 0.0315 - acc: 0.9905 - val_loss: 0.0532 - val_acc: 0.9923\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 57s 955us/step - loss: 0.0311 - acc: 0.9905 - val_loss: 0.0374 - val_acc: 0.9944\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 57s 951us/step - loss: 0.0317 - acc: 0.9903 - val_loss: 0.0373 - val_acc: 0.9941\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 57s 949us/step - loss: 0.0307 - acc: 0.9908 - val_loss: 0.0431 - val_acc: 0.9939\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 89s 1ms/step - loss: 0.0303 - acc: 0.9905 - val_loss: 0.0280 - val_acc: 0.9939\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 79s 1ms/step - loss: 0.0314 - acc: 0.9905 - val_loss: 0.0359 - val_acc: 0.9950\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 58s 970us/step - loss: 0.0304 - acc: 0.9909 - val_loss: 0.0365 - val_acc: 0.9953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 272us/step\n",
      "\n",
      "Test accuracy: 99.5%\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/AppliedDataSciencePartners/DeepReinforcementLearning/issues/3\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# from sparse label to categorical\n",
    "num_labels = len(np.unique(y_train))\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# reshape and normalize input images\n",
    "image_size = x_train.shape[1]\n",
    "x_train = np.reshape(x_train,[-1, image_size, image_size, 1])\n",
    "x_test = np.reshape(x_test,[-1, image_size, image_size, 1])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# network parameters\n",
    "input_shape = (image_size, image_size, 1)\n",
    "batch_size = 32\n",
    "kernel_size = 3\n",
    "dropout = 0.4\n",
    "n_filters = 32\n",
    "\n",
    "# left branch of Y network\n",
    "left_inputs = Input(shape=input_shape)\n",
    "x = left_inputs\n",
    "filters = n_filters\n",
    "# 3 layers of Conv2D-Dropout-MaxPooling2D\n",
    "# number of filters doubles after each layer (32-64-128)\n",
    "for i in range(3):\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=kernel_size,\n",
    "               padding='same',\n",
    "               activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    filters *= 2\n",
    "\n",
    "# right branch of Y network\n",
    "right_inputs = Input(shape=input_shape)\n",
    "y = right_inputs\n",
    "filters = n_filters\n",
    "# 3 layers of Conv2D-Dropout-MaxPooling2D\n",
    "# number of filters doubles after each layer (32-64-128)\n",
    "for i in range(3):\n",
    "    y = Conv2D(filters=filters,\n",
    "               kernel_size=kernel_size,\n",
    "               padding='same',\n",
    "               activation='relu',\n",
    "               dilation_rate=2)(y)\n",
    "    y = Dropout(dropout)(y)\n",
    "    y = MaxPooling2D()(y)\n",
    "    filters *= 2\n",
    "\n",
    "# merge left and right branches outputs\n",
    "y = concatenate([x, y])\n",
    "# feature maps to vector in preparation to connecting to Dense layer\n",
    "y = Flatten()(y)\n",
    "y = Dropout(dropout)(y)\n",
    "outputs = Dense(num_labels, activation='softmax')(y)\n",
    "\n",
    "# build the model in functional API\n",
    "model = Model([left_inputs, right_inputs], outputs)\n",
    "# verify the model using graph\n",
    "plot_model(model, to_file='cnn-y-network.png', show_shapes=True)\n",
    "# verify the model using layer text description\n",
    "model.summary()\n",
    "\n",
    "# classifier loss, Adam optimizer, classifier accuracy\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train the model with input images and labels\n",
    "model.fit([x_train, x_train],\n",
    "          y_train, \n",
    "          validation_data=([x_test, x_test], y_test),\n",
    "          epochs=20,\n",
    "          batch_size=batch_size)\n",
    "\n",
    "# model accuracy on test dataset\n",
    "score = model.evaluate([x_test, x_test], y_test, batch_size=batch_size)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
